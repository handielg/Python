{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semana_4_Parte_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbNHzRiB+EpQzGJk6j8XQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/handielg/Python/blob/main/Semana_4_Parte_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwLZg3FHHEDP"
      },
      "source": [
        "Ahora trabajaremos con imágenes no tan bien estruturadas. Una de las características del generador de imágenes de Tensorflow es que podemos dirigirnos a un directorio, luego a subdirectorios, generando etiquetas para nosotros. Por ejeplo, tenemos una carpeta o directorio de imágenes y en este tenemos subdirectorios de entrenamiento y validación, cuando ponemos nombre a estos subdirectorios, y ponemos las imágenes de cada grupo dentro de estos, el generador de imágenes puede crear un alimentador para estas imágenes y las etiqueta automáticamente.\n",
        "\n",
        "Por ejemplo si dirijo un generador de imágenes al directorio de entrenamiento, con dos carpetas que digan humanos y caballos, todas las imágenes serán cargadas y etiquetadas como corresponde. Lo mismo podemos hacer con el directorio de entrenamiento.\n",
        "\n",
        "Veamoslo con código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28IM9fq_M54e"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBtuzjzOHAY6"
      },
      "source": [
        "#La clase generador de imágenes esta disponible con este código.\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4ubB_sQJvjF"
      },
      "source": [
        "Podemos entonces instanciar un generador de imágenes de esta forma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JimnwNsKJp87"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255) #reescalamos para normalizar los datos.\n",
        "\n",
        "#El nombre de los subdirectorios que tengamos, serán los nombres de las etiquetas de las imágenes.\n",
        "#Llmamamos al método flow_from_directory para que cargue las imágenes de ese directorio y los subdirectorios. No lo debemos dirigir al subdirectorio.\n",
        "# Imágenes de entrenamiento de flujo en lotes de 128 usando el generador train_datagen\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # Este es el directorio de origen para las imágenes de entrenamiento.\n",
        "        target_size=(300, 300),  # Todas las imágenes cambiarán de tamaño a 300x300, las redimensionamos al cargarlas. Esto no afecta las imágenes originales.\n",
        "        batch_size=128, #tamaño del lote.\n",
        "       # Dado que usamos pérdida binary_crossentropy, necesitamos etiquetas binarias\n",
        "        class_mode='binary') #para que elija entre 2 cosas, mas adelante veremos otras."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Z62_xJKj3Z"
      },
      "source": [
        "Como podemos ver este código es similar al de la semana pasada. En este caso hay más capas de convlución y pooling, por el tamaño y complejidad de las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjimUXw8ML_9"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Tenga en cuenta que la forma de entrada es el tamaño deseado de la imagen 300x300 con 3 bytes de color\n",
        "    # Esta es la primera convolución\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)), #como son de colores, las redimensionamos a 3 bytes por pixel, 1 para el rojo, 1 azul y 1 para el verde.\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Solo 1 neurona de salida. Contendrá un valor de 0-1 donde 0 para 1 clase ('caballos') y 1 para la otra ('humanos')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid') #solo una neurona para 2 clases. Usamos la función sigmoide porque es muy buena para clasificación binaria. Cuando una clase tiende a 0, la otra a 1.\n",
        "])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hVH90bQM8cK",
        "outputId": "2df7ff91-aa79-4fec-c1de-716dafb59643"
      },
      "source": [
        "model.summary() #como podmeos ver terminamos en imágenes de 7x7. Una vez que los datos salen de las convoluciones, son aplanados a 3136 valores."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZFqEovfOtYm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-_GFk0lOsE8"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy', #función de pérdida. Escogemos esta porque es clasificación binaria.\n",
        "              optimizer=RMSprop(lr=0.001), #optimizador. Con este podemos ajustar la tasa de aprendizaje para experimentar con el rendimiento.\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYXZGmz3PfVt"
      },
      "source": [
        "###Entrenamiento\n",
        "\n",
        "Entrenemos durante 15 épocas; esto puede tardar unos minutos en ejecutarse.\n",
        "\n",
        "Tenga en cuenta los valores por época.\n",
        "\n",
        "La pérdida y la precisión son una gran indicación del progreso del entrenamiento. Se trata de adivinar la clasificación de los datos de entrenamiento y luego compararlos con la etiqueta conocida para calcular el resultado. La precisión es la parte de las conjeturas correctas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xod3qAcMPheb"
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator, #este es el train_generator que configuramos anteriormente. Este hace fluir las imágenes desde el directorio. \n",
        "      steps_per_epoch=8, #dijimos que batch_size = 128, debemos dividir la cantidad total de imágenes entre 128 para encontrar steps_per_epoch.\n",
        "      epochs=15, #número de epoch para entrenar.\n",
        "      verbose=2, #especifica cuanto se muestra en el entrenamiento, en 2, obtenemos menos animación ocultando el progreso del epoch. Con 1 más.\n",
        "      validation_data = validation_generator, #especificamos el conjunto de validación.\n",
        "      validation_steps=8) #si tenemos 256 imágenes y las usaremos en lotes de 32, necesitamos 8 pasos."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d4ZQamARwqe"
      },
      "source": [
        "###Ejecutando el modelo\n",
        "Echemos ahora un vistazo a la ejecución real de una predicción utilizando el modelo. Este código le permitirá elegir 1 o más archivos de su sistema de archivos, luego los cargará y los ejecutará a través del modelo, dando una indicación de si el objeto es un caballo o un humano."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdWXQcqeRyzg"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files #nos permite importar datos.\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload() #la ruta de la imagen se carga en este archivo.\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "#El bucle itera a través de todas las imágenes de esta colección.\n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10) #este devolverá un vector de clases. Como es binario tendrá valores cercanos a 0 para una clase, y cercano a 1 para la otra.\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8flvXo6PTFZA"
      },
      "source": [
        "#**Ejemplo completo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTfS-NndTNk9",
        "outputId": "93b3c474-1a83-481c-da2e-ce7f157a9c2e"
      },
      "source": [
        "#descargamos el archivo zip que contiene las imágenes.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-01 17:33:01--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.2.112, 142.250.73.208, 172.217.7.240, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.2.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘/tmp/horse-or-human.zip’\n",
            "\n",
            "/tmp/horse-or-human 100%[===================>] 142.65M  93.4MB/s    in 1.5s    \n",
            "\n",
            "2020-12-01 17:33:02 (93.4 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Objk-5GMTfX6"
      },
      "source": [
        "El siguiente código de Python usará la biblioteca del sistema operativo para usar las bibliotecas del sistema operativo, lo que le dará acceso al sistema de archivos y la biblioteca zipfile le permitirá descomprimir los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVDIoS8TYw1"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "#Descomprimimos el archivo zip.\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "zip_ref.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmphr4SBTpT2"
      },
      "source": [
        "El contenido del .zip se extrae al directorio base / tmp / horse-or-human, que a su vez contiene subdirectorios de caballos y humanos.\n",
        "\n",
        "En resumen: el conjunto de entrenamiento son los datos que se utilizan para decirle al modelo de red neuronal que 'así es como se ve un caballo', 'así es como se ve un humano', etc.\n",
        "\n",
        "Una cosa a la que hay que prestar atención en esta muestra: no etiquetamos explícitamente las imágenes como caballos o humanos. Si recuerda el ejemplo de escritura a mano anterior, habíamos etiquetado 'esto es un 1', 'esto es un 7', etc. Más tarde verá algo llamado ImageGenerator en uso, y esto está codificado para leer imágenes de subdirectorios, y etiquetarlos automáticamente con el nombre de ese subdirectorio. Entonces, por ejemplo, tendrá un directorio de 'entrenamiento' que contiene un directorio de 'caballos' y uno de 'humanos'. ImageGenerator etiquetará las imágenes apropiadamente para usted, reduciendo un paso de codificación.\n",
        "\n",
        "Definamos cada uno de estos directorios:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaRi_doKTuSI"
      },
      "source": [
        "#El archivo zip contiene 2 carpetas, creamos las variables de entrenamiento.\n",
        "\n",
        "# Directorio con nuestras fotos de caballos de entrenamiento.\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "# Directorio con nuestras imágenes de entrenamiento humano.\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THYCuixtUErB"
      },
      "source": [
        "Ahora, veamos cómo se ven los nombres de los archivos en los directorios de entrenamiento de caballos y humanos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tZrsXg6UI09",
        "outputId": "3bdf27b9-1736-4bf1-9619-780b92186046"
      },
      "source": [
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "print(train_horse_names[:10])\n",
        "\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "print(train_human_names[:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['horse31-1.png', 'horse24-1.png', 'horse27-8.png', 'horse46-9.png', 'horse44-3.png', 'horse37-6.png', 'horse28-5.png', 'horse41-8.png', 'horse06-7.png', 'horse12-2.png']\n",
            "['human01-15.png', 'human03-00.png', 'human07-11.png', 'human10-02.png', 'human03-19.png', 'human09-22.png', 'human15-27.png', 'human12-25.png', 'human15-03.png', 'human16-06.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRRKGWX2UNq3"
      },
      "source": [
        "Averigüemos el número total de imágenes de caballos y humanos en los directorios:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xRlluoDUR53",
        "outputId": "5a63efa7-4069-4f0c-a303-f18fbb8a17d7"
      },
      "source": [
        "print('total training horse images:', len(os.listdir(train_horse_dir)))\n",
        "print('total training human images:', len(os.listdir(train_human_dir)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training horse images: 500\n",
            "total training human images: 527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH69vBXrUX28"
      },
      "source": [
        "\n",
        "Ahora echemos un vistazo a algunas imágenes para tener una mejor idea de cómo se ven. Primero, configure los parámetros de matplot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hif3q8LRUY1_"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parámetros para nuestro gráfico; emitiremos imágenes en una configuración 4x4\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "# Índice para iterar sobre imágenes\n",
        "pic_index = 0"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPWikXxTUrFq"
      },
      "source": [
        "Ahora, muestre un lote de 8 imágenes de caballos y 8 humanas. Puede volver a ejecutar la celda para ver un lote nuevo cada vez:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZs8GRbdUtSe"
      },
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
        "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
        "next_human_pix = [os.path.join(train_human_dir, fname) \n",
        "                for fname in train_human_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWKCyckCUxh_"
      },
      "source": [
        "###Construyendo un modelo pequeño desde cero¶\n",
        "Pero antes de continuar, comencemos a definir el modelo:\n",
        "\n",
        "El paso 1 será importar tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE8_Sv0NU4cI"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbX3qHD6U97c"
      },
      "source": [
        "Luego agregamos capas convolucionales como en el ejemplo anterior y aplanamos el resultado final para alimentar las capas densamente conectadas.\n",
        "\n",
        "Finalmente agregamos las capas densamente conectadas.\n",
        "\n",
        "Tenga en cuenta que debido a que nos enfrentamos a un problema de clasificación de dos clases, es decir, un problema de clasificación binaria, terminaremos nuestra red con una activación sigmoidea, de modo que la salida de nuestra red será un solo escalar entre 0 y 1, codificando la probabilidad de que la imagen actual es de clase 1 (a diferencia de la clase 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNaTF3kAU-vm"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbyPOrMPVFSv"
      },
      "source": [
        "La llamada al método model.summary () imprime un resumen de la NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwUjtqG3VF5A",
        "outputId": "92fb6e40-4b78-4c58-8bca-96dff483d66a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stFTfx_wVPrU"
      },
      "source": [
        "La columna \"forma de salida\" muestra cómo evoluciona el tamaño de su mapa de características en cada capa sucesiva. Las capas de convolución reducen un poco el tamaño de los mapas de características debido al relleno, y cada capa de agrupación reduce a la mitad las dimensiones.\n",
        "\n",
        "A continuación, configuraremos las especificaciones para el entrenamiento de modelos. Entrenaremos nuestro modelo con la pérdida de binary_crossentropy, porque es un problema de clasificación binaria y nuestra activación final es sigmoidea. (Para un repaso sobre las métricas de pérdida, consulte el [Curso intensivo de aprendizaje automático](https://developers.google.com/machine-learning/crash-course/descending-into-ml/video-lecture)). Usaremos el optimizador rmsprop con una tasa de aprendizaje de 0,001. Durante el entrenamiento, queremos monitorear la precisión de la clasificación.\n",
        "\n",
        "NOTA: En este caso, usar el algoritmo de optimización RMSprop es preferible al descenso de gradiente estocástico [SGD](https://developers.google.com/machine-learning/glossary/#SGD), porque [RMSprop](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) automatiza el ajuste de la tasa de aprendizaje para nosotros. (Otros optimizadores, como [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) y [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad), también adaptan automáticamente la tasa de aprendizaje durante el entrenamiento y funcionarían igualmente bien aquí)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3-QcpIMWKuj"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jiMXCzeWMoy"
      },
      "source": [
        "###Preprocesamiento de datos.\n",
        "\n",
        "Configuremos generadores de datos que leerán imágenes en nuestras carpetas de origen, las convertirán en tensores float32 y las alimentarán (con sus etiquetas) a nuestra red. Tendremos un generador para las imágenes de entrenamiento y otro para las imágenes de validación. Nuestros generadores producirán lotes de imágenes de tamaño 300x300 y sus etiquetas (binarias).\n",
        "\n",
        "Como ya sabrá, los datos que ingresan a las redes neuronales generalmente deben normalizarse de alguna manera para que sean más fáciles de procesar por la red. (Es poco común introducir píxeles sin procesar en una convnet). En nuestro caso, procesaremos previamente nuestras imágenes normalizando los valores de los píxeles para que estén en el rango [0, 1] (originalmente todos los valores están en el rango [0, 255] ).\n",
        "\n",
        "En Keras, esto se puede hacer a través de la clase keras.preprocessing.image.ImageDataGenerator usando el parámetro rescale. Esta clase ImageDataGenerator le permite crear instancias de generadores de lotes de imágenes aumentadas (y sus etiquetas) a través de .flow (datos, etiquetas) o .flow_from_directory (directorio). Estos generadores se pueden usar con los métodos del modelo de Keras que aceptan generadores de datos como entradas: ajuste, evalúa_generador y predice_generador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rpaTPzyWRMn",
        "outputId": "60c50bd7-d113-480b-aee0-fc65bf88c886"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 150x150\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCAGogKhWZy-"
      },
      "source": [
        "###Entrenamiento\n",
        "\n",
        "Entrenemos durante 15 épocas; esto puede tardar unos minutos en ejecutarse.\n",
        "\n",
        "Tenga en cuenta los valores por época.\n",
        "\n",
        "La pérdida y la precisión son una gran indicación del progreso del entrenamiento. Se trata de adivinar la clasificación de los datos de entrenamiento y luego compararlos con la etiqueta conocida para calcular el resultado. La precisión es la parte de las conjeturas correctas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqPqO1TCXBox",
        "outputId": "9f6f6f5b-e6c7-4830-9c09-c0da5a2ad8fd"
      },
      "source": [
        "#Solicitamos GPU\n",
        "%tensorflow_version 2.x\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0': #esto es lo que nos debe salir si nos asignan GPU.\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhZaYchBWgK5",
        "outputId": "ae147091-9e29-4f3f-adb6-579df7828c3a"
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "2/8 [======>.......................] - ETA: 0s - loss: 1.2957 - accuracy: 0.5078WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0635s vs `on_train_batch_end` time: 0.1038s). Check your callbacks.\n",
            "8/8 [==============================] - 5s 608ms/step - loss: 0.8622 - accuracy: 0.5495\n",
            "Epoch 2/15\n",
            "8/8 [==============================] - 6s 692ms/step - loss: 1.1627 - accuracy: 0.5907\n",
            "Epoch 3/15\n",
            "8/8 [==============================] - 5s 672ms/step - loss: 0.5680 - accuracy: 0.7075\n",
            "Epoch 4/15\n",
            "8/8 [==============================] - 5s 667ms/step - loss: 0.3813 - accuracy: 0.8309\n",
            "Epoch 5/15\n",
            "8/8 [==============================] - 5s 679ms/step - loss: 0.6887 - accuracy: 0.8354\n",
            "Epoch 6/15\n",
            "8/8 [==============================] - 5s 678ms/step - loss: 0.6850 - accuracy: 0.8198\n",
            "Epoch 7/15\n",
            "8/8 [==============================] - 5s 678ms/step - loss: 0.4321 - accuracy: 0.8788\n",
            "Epoch 8/15\n",
            "8/8 [==============================] - 5s 682ms/step - loss: 0.1468 - accuracy: 0.9488\n",
            "Epoch 9/15\n",
            "8/8 [==============================] - 5s 674ms/step - loss: 0.1194 - accuracy: 0.9511\n",
            "Epoch 10/15\n",
            "8/8 [==============================] - 5s 680ms/step - loss: 0.9793 - accuracy: 0.8265\n",
            "Epoch 11/15\n",
            "8/8 [==============================] - 5s 675ms/step - loss: 0.1514 - accuracy: 0.9533\n",
            "Epoch 12/15\n",
            "8/8 [==============================] - 5s 665ms/step - loss: 0.0701 - accuracy: 0.9789\n",
            "Epoch 13/15\n",
            "8/8 [==============================] - 5s 671ms/step - loss: 0.0837 - accuracy: 0.9666\n",
            "Epoch 14/15\n",
            "8/8 [==============================] - 5s 674ms/step - loss: 0.0427 - accuracy: 0.9778\n",
            "Epoch 15/15\n",
            "8/8 [==============================] - 5s 680ms/step - loss: 0.2517 - accuracy: 0.9055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXJ3f6XvWr58"
      },
      "source": [
        "###Ejecutando el modelo\n",
        "Echemos ahora un vistazo a la ejecución real de una predicción utilizando el modelo. Este código le permitirá elegir 1 o más archivos de su sistema de archivos, luego los cargará y los ejecutará a través del modelo, dando una indicación de si el objeto es un caballo o un humano."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "b-_L-WAjWuIV",
        "outputId": "9ad4f491-3015-4db7-c324-df3bf3283be9"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" es una persona\")\n",
        "  else:\n",
        "    print(fn + \" es un caballo\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cfe20751-b850-469c-aaad-2c28302ac23a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cfe20751-b850-469c-aaad-2c28302ac23a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dfghjs.jpg to dfghjs (1).jpg\n",
            "[0.]\n",
            "dfghjs.jpg es un caballo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mj7PoGsWxBT"
      },
      "source": [
        "###Visualización de representaciones intermedias¶\n",
        "Para tener una idea de qué tipo de características ha aprendido nuestro convnet, una cosa divertida es visualizar cómo se transforma una entrada a medida que pasa por el convnet.\n",
        "\n",
        "Escojamos una imagen aleatoria del conjunto de entrenamiento y luego generemos una figura donde cada fila es el resultado de una capa, y cada imagen de la fila es un filtro específico en ese mapa de características de salida. Vuelva a ejecutar esta celda para generar representaciones intermedias para una variedad de imágenes de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jggbz-ptW262"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "#visualization_model = Model(img_input, successive_outputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers[1:]]\n",
        "\n",
        "# Now let's display our representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGAtGCljXsd_"
      },
      "source": [
        "Como ves pasamos de los píxeles en bruto de las imágenes a representaciones cada vez más abstractas y compactas. Las representaciones en sentido descendente comienzan a resaltar aquello a lo que la red presta atención y muestran cada vez menos funciones que se \"activan\"; la mayoría se ponen a cero. A esto se le llama \"escasez\". La escasez de representación es una característica clave del aprendizaje profundo.\n",
        "\n",
        "Estas representaciones llevan cada vez menos información sobre los píxeles originales de la imagen, pero información cada vez más refinada sobre la clase de la imagen. Puede pensar en un convnet (o una red profunda en general) como una tubería de destilación de información.\n",
        "\n",
        "###Limpiar\n",
        "\n",
        "Antes de ejecutar el siguiente ejercicio, ejecute la siguiente celda para terminar el kernel y liberar recursos de memoria:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5S5GHBrXvEz"
      },
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}